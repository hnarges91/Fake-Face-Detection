{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hnarges91/Fake-Face-Detection/blob/main/deepfake_detection_ipyn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIEksLT7atwb"
      },
      "outputs": [],
      "source": [
        "#setup\n",
        "#reset variavle value\n",
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf')\n",
        "#intellisence\n",
        "%config IPCcompleter.greedy = True\n",
        "import numpy as np\n",
        "np.random.seed(400)  # for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hliTXdDejPol",
        "outputId": "031cf06a-ef22-4a20-ae12-d2248cf90909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/My Drive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzG8i9Q8zLwA"
      },
      "outputs": [],
      "source": [
        "# #download the dataset\n",
        "# !unzip '/content/drive/My Drive/datasets/archive.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItcopAkJpfEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54427518-c545-47c4-f8ea-92b041372e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "import os  # Import the os module to interact with the operating system's file system\n",
        "\n",
        "# Define the directory path for the 'real' images\n",
        "directory_path = \"/content/drive/My Drive/datasets/data_140k/real/\"\n",
        "\n",
        "# Get a sorted list of all files in the specified directory\n",
        "Images = sorted([vfile for vfile in (os.listdir(directory_path))])\n",
        "\n",
        "# Print the number of 'real' images found in the directory\n",
        "print(len(Images))\n",
        "\n",
        "# Define the directory path for the 'fake' images\n",
        "directory_path = \"/content/drive/My Drive/datasets/data_140k/fake/\"\n",
        "\n",
        "# Get a sorted list of all files in the specified directory\n",
        "Images2 = sorted([vfile for vfile in os.listdir(directory_path)])\n",
        "\n",
        "# Print the number of 'fake' images found in the directory\n",
        "print(len(Images2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnChDXjD4M8H"
      },
      "source": [
        "# pretrain models selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oM5omnNbkpf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Flatten, Dense  # Import layers from Keras\n",
        "from tensorflow.keras.models import Model  # Import Model class from Keras\n",
        "from tensorflow.keras import applications  # Import applications module from Keras\n",
        "\n",
        "import tensorflow as tf  # Import TensorFlow library\n",
        "\n",
        "# Import VGG16 model and preprocessing functions from Keras\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# Import VGG19 model and preprocessing functions from Keras\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "\n",
        "# Import ResNet50 model and preprocessing functions from Keras\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "def choose_model(model_name):\n",
        "    \"\"\"\n",
        "    Choose and configure the specified pre-trained model for feature extraction.\n",
        "\n",
        "    Parameters:\n",
        "        model_name (str): Name of the pre-trained model to use.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the full model and the custom feature extraction model.\n",
        "    \"\"\"\n",
        "    print(\"Selecting model...\")  # Print status message\n",
        "\n",
        "    # Define input tensor shape\n",
        "    input_tensor = Input(shape=(256, 256, 3))\n",
        "\n",
        "    # Check if the model_name is for VGG16\n",
        "    if model_name in ['vgg16', 'finetunevgg16']:\n",
        "        # Load the VGG16 model without the top layers, using the input tensor defined\n",
        "        model = applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "        # Create a custom model for feature extraction, ending at a specific layer\n",
        "        custom_model = Model(model.inputs, model.layers[-5].output)\n",
        "\n",
        "    # Check if the model_name is for VGG19\n",
        "    elif model_name in ['vgg19', 'finetunevgg19']:\n",
        "        # Load the VGG19 model without the top layers, using the input tensor defined\n",
        "        model = applications.VGG19(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "        # Create a custom model for feature extraction, ending at a specific layer\n",
        "        custom_model = Model(model.inputs, model.layers[-6].output)\n",
        "\n",
        "    # Check if the model_name is for ResNet50\n",
        "    elif model_name in ['resnet50', 'finetuneresnet50']:\n",
        "        # Load the ResNet50 model without the top layers, using the input tensor defined\n",
        "        model = applications.ResNet50(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "        # Create a custom model for feature extraction, ending at a specific layer\n",
        "        custom_model = Model(model.inputs, model.layers[81].output)\n",
        "\n",
        "    else:\n",
        "        # Print error message if the model_name is not supported\n",
        "        print(f\"Error: Unsupported model '{model_name}'.\")\n",
        "        return None, None  # Return None if the model is not supported\n",
        "\n",
        "    print(\"Model selected.\")  # Print status message\n",
        "\n",
        "    return custom_model  # Return the custom feature extraction model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp0ADn-b6sIw"
      },
      "source": [
        "# read Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWQQlUWxrhCx"
      },
      "outputs": [],
      "source": [
        "import cv2  # Import OpenCV for image processing\n",
        "import numpy as np  # Import NumPy for numerical operations\n",
        "\n",
        "def Image_reader(path, class_name):\n",
        "    \"\"\"\n",
        "    Read images from the specified dataset directory.\n",
        "\n",
        "    Parameters:\n",
        "        path (str): Path to the dataset directory.\n",
        "        class_name (str): Name of the class containing images.\n",
        "\n",
        "    Returns:\n",
        "        list: List of images read from the dataset.\n",
        "    \"\"\"\n",
        "    print(\"Reading images...\")  # Print status message\n",
        "\n",
        "    # Initialize an empty list to store images\n",
        "    images = []\n",
        "\n",
        "    # Specify the directory path for the given class\n",
        "    directory_path = os.path.join(path, class_name)\n",
        "\n",
        "    # Check if the directory exists\n",
        "    if not os.path.exists(directory_path):\n",
        "        # Print error message if the directory does not exist\n",
        "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
        "        return []  # Return an empty list\n",
        "\n",
        "    # Iterate through files in the directory\n",
        "    for filename in os.listdir(directory_path):\n",
        "        # Create the full path to the file\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Check if the path is a file (not a directory) and if it's an image file\n",
        "        if os.path.isfile(file_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            # Read the image\n",
        "            image = cv2.imread(file_path)\n",
        "\n",
        "            # Check if the image was successfully read\n",
        "            if image is not None:\n",
        "                # Resize the image to a standard size (256x256)\n",
        "                image = cv2.resize(image, (256, 256))\n",
        "                # Append the image to the list\n",
        "                images.append(image)\n",
        "\n",
        "    # Print the number of images read\n",
        "    print(f\"Read {len(images)} images.\")\n",
        "\n",
        "    return images  # Return the list of images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVFRL2--2v7m"
      },
      "source": [
        "# Feature extracting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI21z62QaygC"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Import NumPy for numerical operations\n",
        "import os  # Import the os module to interact with the operating system's file system\n",
        "\n",
        "def feature_extractor_image(path, model_name, class_name, feature_kind):\n",
        "    \"\"\"\n",
        "    Extract features from images using the specified model and feature type.\n",
        "\n",
        "    Parameters:\n",
        "        path (str): Path to the images.\n",
        "        model_name (str): Name of the model to use.\n",
        "        class_name (str): Name of the class.\n",
        "        feature_kind (str): Type of feature extraction method.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Extracted features.\n",
        "    \"\"\"\n",
        "    # Select the appropriate model for feature extraction\n",
        "    custom_model2 = choose_model(model_name)\n",
        "    if custom_model2 is None:\n",
        "        # Log an error if the model fails to load\n",
        "        print(\"Error: Failed to load model.\")\n",
        "        return None\n",
        "\n",
        "    # Load images from the specified dataset and class\n",
        "    frames_list = Image_reader(path, class_name)\n",
        "    print(\"Feature extracting...\")\n",
        "\n",
        "    # Convert the list of images to a NumPy array\n",
        "    img_data = np.asarray(frames_list)\n",
        "    del frames_list  # Release memory by deleting the original list\n",
        "\n",
        "    if feature_kind == \"correlation\":\n",
        "        # Initialize an empty list to store extracted features\n",
        "        frame_features = []\n",
        "        for img in img_data:\n",
        "            # Clear the Keras backend session to free up memory\n",
        "            keras.backend.clear_session()\n",
        "            # Expand dimensions of the image to match the model input shape\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "            # Preprocess the image for the model\n",
        "            img = preprocess_input(img)\n",
        "            # Predict features using the custom model\n",
        "            features = custom_model2.predict(img)\n",
        "            # Remove the batch dimension from the features\n",
        "            features = np.reshape(features, np.shape(features)[1:])\n",
        "            # Log the shape of the extracted features\n",
        "            print(np.shape(features))\n",
        "            # Compute the correlation-based features\n",
        "            frame_features.append(calculate_correlation(features))\n",
        "            # Log the current shape of the frame features list\n",
        "            print(\"frame_features\", np.shape(frame_features))\n",
        "            # Free up memory by deleting intermediate variables\n",
        "            del features, img\n",
        "\n",
        "    # Log the final shape of the extracted features\n",
        "    print(np.shape(frame_features))\n",
        "    # Convert the list of frame features to a NumPy array\n",
        "    frame_features = np.array(frame_features)\n",
        "    return frame_features  # Return the extracted features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCCRbIA57CqB"
      },
      "source": [
        "# correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy_8NlJUPDi2"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Import NumPy for numerical operations\n",
        "\n",
        "def calculate_correlation(feature_map):\n",
        "    \"\"\"\n",
        "    Calculate the correlation matrix for each matrix in the feature map, removing the upper triangular part.\n",
        "\n",
        "    Parameters:\n",
        "    - feature_map (numpy.ndarray): Feature map with shape (m, n, c), where c is the number of channels.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: Lower triangular part of the correlation matrix for each channel in the feature map.\n",
        "    \"\"\"\n",
        "    # Get the shape of the feature map\n",
        "    m, n, c = feature_map.shape\n",
        "\n",
        "    # Initialize a list to store the flattened matrices for each channel\n",
        "    flattened_matrices = []\n",
        "\n",
        "    # Iterate over each channel\n",
        "    for j in range(c):\n",
        "        keras.backend.clear_session()  # Clear Keras backend session to free up memory\n",
        "\n",
        "        # Extract the matrix for channel j and flatten it\n",
        "        matrix = feature_map[:, :, j].flatten()\n",
        "        flattened_matrices.append(matrix)  # Append the flattened matrix to the list\n",
        "\n",
        "    # Convert the list of flattened matrices to a NumPy array\n",
        "    flattened_matrices = np.array(flattened_matrices)\n",
        "\n",
        "    # Transpose the matrix to prepare for dot product\n",
        "    transposed_matrix = flattened_matrices.T\n",
        "\n",
        "    # Calculate the dot product of the matrix with its transpose\n",
        "    correlation_matrix = np.dot(flattened_matrices, transposed_matrix)\n",
        "    print(\"Correlation matrix shape:\", np.shape(correlation_matrix))\n",
        "\n",
        "    # Delete intermediate variables to free up memory\n",
        "    del matrix, transposed_matrix, flattened_matrices\n",
        "\n",
        "    # Get the indices for the lower triangular part of the matrix\n",
        "    lower_indices = np.tril_indices(c)\n",
        "\n",
        "    # Extract the lower triangular part of the correlation matrix\n",
        "    lower_triangular = correlation_matrix[lower_indices]\n",
        "    # print(\"Lower triangular shape:\", np.shape(lower_triangular))\n",
        "\n",
        "    # Delete the correlation matrix to free up memory\n",
        "    del correlation_matrix\n",
        "\n",
        "    return lower_triangular  # Return the lower triangular part of the correlation matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5F0-H0Elc2z"
      },
      "source": [
        "# Dataset's Address"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWnLmYMPeE4e"
      },
      "outputs": [],
      "source": [
        "def dataset_address(dataset_name):\n",
        "  if dataset_name=='140k':\n",
        "    return \"/content/drive/My Drive/datasets/data_140k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF_X2DpUlw5p"
      },
      "outputs": [],
      "source": [
        "def featurs_address():\n",
        "    return '/content/drive/My Drive/datasets/features/ten/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7IMh4h62oUN"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4gFxVLwdNDb"
      },
      "outputs": [],
      "source": [
        "import os  # Import the os module to interact with the operating system's file system\n",
        "import numpy as np  # Import NumPy for numerical operations\n",
        "\n",
        "def feature_loader(dataset_name, model_name, feature_kind):\n",
        "    \"\"\"\n",
        "    Load pre-computed features from files if available, otherwise extract features using the specified model.\n",
        "\n",
        "    Parameters:\n",
        "        dataset_name (str): Name of the dataset.\n",
        "        model_name (str): Name of the model used for feature extraction.\n",
        "        feature_kind (str): Type of features to load or extract.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing the loaded real and fake features.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Determine the feature address based on the provided layer\n",
        "        feature_address = featurs_address()\n",
        "    except Exception as e:\n",
        "        # Log an error message if determining the feature address fails\n",
        "        print(f\"An error occurred while determining the feature address: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        # Initialize variables to store features\n",
        "        real_feature, fake_feature = None, None\n",
        "\n",
        "        # Load real features\n",
        "        label = 0  # Label for real class\n",
        "        class_name = 'real'  # Class name for real images\n",
        "        print(f\"Loading real features for '{dataset_name}'...\")\n",
        "        feature_filename = f\"{dataset_name}_{class_name}_{model_name}_{feature_kind}.npy\"  # Filename for real features\n",
        "        feature_path = os.path.join(feature_address, feature_filename)  # Full path to the feature file\n",
        "\n",
        "        if os.path.isfile(feature_path):\n",
        "            # If the feature file exists, load the features\n",
        "            print(f\"File '{feature_filename}' exists. Loading...\")\n",
        "            real_feature = np.load(feature_path, allow_pickle=True)\n",
        "        else:\n",
        "            # If the feature file does not exist, extract features\n",
        "            print(f\"File '{feature_filename}' does not exist. Extracting features...\")\n",
        "            data_path = dataset_address(dataset_name)  # Get the data path for the dataset\n",
        "            real_feature = feature_extractor_image( data_path, model_name, class_name, feature_kind)\n",
        "            # Save the extracted features\n",
        "            np.save(feature_path, real_feature)\n",
        "        print(f\"Real features loaded. Shape: {np.shape(real_feature)}\")\n",
        "\n",
        "        # Load fake features\n",
        "        label = 1  # Label for fake class\n",
        "        class_name = 'fake'  # Class name for fake images\n",
        "        print(f\"Loading fake features for '{dataset_name}'...\")\n",
        "        feature_filename = f\"{dataset_name}_{class_name}_{model_name}_{feature_kind}.npy\"  # Filename for fake features\n",
        "        feature_path = os.path.join(feature_address, feature_filename)  # Full path to the feature file\n",
        "\n",
        "        if os.path.isfile(feature_path):\n",
        "            # If the feature file exists, load the features\n",
        "            print(f\"File '{feature_filename}' exists. Loading...\")\n",
        "            fake_feature = np.load(feature_path, allow_pickle=True)\n",
        "        else:\n",
        "            # If the feature file does not exist, extract features\n",
        "            print(f\"File '{feature_filename}' does not exist. Extracting features...\")\n",
        "            data_path = dataset_address(dataset_name)  # Get the data path for the dataset\n",
        "            fake_feature = feature_extractor_image( data_path, model_name, class_name, feature_kind)\n",
        "            # Save the extracted features\n",
        "            np.save(feature_path, fake_feature)\n",
        "        print(f\"Fake features loaded. Shape: {np.shape(fake_feature)}\")\n",
        "\n",
        "        return real_feature, fake_feature  # Return the loaded real and fake features\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log an error message if any exception occurs\n",
        "        print(f\"An error occurred during feature loading: {e}\")\n",
        "        return None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24novW7UlU-i"
      },
      "source": [
        "# Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4dRdkwkz7-W"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Import NumPy for numerical operations\n",
        "import tensorflow.keras.backend as K  # Import Keras backend for session management\n",
        "\n",
        "def labeling(real_test, fake_test):\n",
        "    \"\"\"\n",
        "    Label the test features and combine them into a single dataset.\n",
        "\n",
        "    Parameters:\n",
        "        real_test (numpy.ndarray): Array of features for real images.\n",
        "        fake_test (numpy.ndarray): Array of features for fake images.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing the labeled test features (X_test) and their corresponding labels (y_test).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Clear the Keras backend session to free up memory\n",
        "        K.clear_session()\n",
        "        print(\"********** Labeling **********\")\n",
        "\n",
        "        # Label real images with 0\n",
        "        real_label_test = np.zeros((real_test.shape[0], 1))\n",
        "        print(\"Real labels:\", real_label_test.shape)\n",
        "\n",
        "        # Label fake images with 1\n",
        "        fake_label_test = np.ones((fake_test.shape[0], 1))\n",
        "        print(\"Fake labels:\", fake_label_test.shape)\n",
        "\n",
        "        # Combine real and fake features\n",
        "        features_combined = np.concatenate((real_test, fake_test), axis=0)\n",
        "        # Combine real and fake labels\n",
        "        labels_combined = np.concatenate((real_label_test, fake_label_test))\n",
        "\n",
        "        # Shuffle the combined dataset\n",
        "        indices = np.arange(features_combined.shape[0])  # Generate indices for the dataset\n",
        "        np.random.shuffle(indices)  # Shuffle the indices\n",
        "        X_test = features_combined[indices]  # Shuffle the features\n",
        "        y_test = labels_combined[indices]  # Shuffle the labels\n",
        "\n",
        "\n",
        "\n",
        "        # Delete intermediate variables to free up memory\n",
        "        del real_test, fake_test, features_combined, labels_combined\n",
        "\n",
        "        # Clear the Keras backend session to free up memory again\n",
        "        K.clear_session()\n",
        "\n",
        "        # Print the shapes of the combined features and labels\n",
        "        print(\"Combined features shape:\", X_test.shape)\n",
        "        print(\"Labels shape:\", y_test.shape)\n",
        "\n",
        "        return X_test, y_test  # Return the combined and shuffled features and labels\n",
        "\n",
        "    except Exception as e:\n",
        "        # Print the error message if an exception occurs\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTZaeL7C2Y9D"
      },
      "source": [
        "# Classificatoin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9YpP4602HjQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras  # Import Keras from TensorFlow\n",
        "from tensorflow.keras.models import Sequential  # Import Sequential model type from Keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization  # Import layers from Keras\n",
        "from tensorflow.keras.utils import to_categorical  # Import utility for one-hot encoding\n",
        "\n",
        "def neural_network_classification(X_train, y_train, input_shape):\n",
        "    \"\"\"\n",
        "    Train a neural network classifier.\n",
        "\n",
        "    Parameters:\n",
        "        X_train (numpy.ndarray): Input features for training.\n",
        "        y_train (numpy.ndarray): Target labels for training.\n",
        "        input_shape (tuple): Shape of the input data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing the training history and the trained model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Clear any previous Keras session to free up memory\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        # Convert labels to one-hot encoded format\n",
        "        y_train = to_categorical(y_train)\n",
        "\n",
        "        # Define the neural network architecture\n",
        "        model = Sequential([\n",
        "            Dense(1024, activation='relu', input_shape=input_shape),  # First dense layer with ReLU activation\n",
        "            BatchNormalization(),  # Batch normalization layer\n",
        "            Dense(1024, activation='relu'),  # Second dense layer with ReLU activation\n",
        "            BatchNormalization(),  # Batch normalization layer\n",
        "            Dense(1024, activation='relu'),  # Third dense layer with ReLU activation\n",
        "            Dense(1024, activation='relu'),  # Fourth dense layer with ReLU activation\n",
        "            Dropout(0.2),  # Dropout layer with 20% dropout rate\n",
        "            Dense(1024, activation='relu'),  # Fifth dense layer with ReLU activation\n",
        "            Dropout(0.2),  # Dropout layer with 20% dropout rate\n",
        "            Dense(1024, activation='relu'),  # Sixth dense layer with ReLU activation\n",
        "            Dropout(0.3),  # Dropout layer with 30% dropout rate\n",
        "            Dense(2, activation='softmax')  # Output layer with softmax activation for binary classification\n",
        "        ])\n",
        "\n",
        "        # Print the model summary to visualize the architecture\n",
        "        model.summary()\n",
        "\n",
        "        # Define callback for early stopping to prevent overfitting\n",
        "        callback = keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_accuracy\",  # Monitor validation accuracy\n",
        "            patience=5,  # Number of epochs with no improvement after which training will be stopped\n",
        "            mode=\"max\",  # Mode for monitoring the accuracy\n",
        "            restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
        "        )\n",
        "\n",
        "        # Compile the model with Adam optimizer and binary cross-entropy loss\n",
        "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=6e-5),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # Train the model with the training data\n",
        "        history = model.fit(X_train, y_train, epochs=200, verbose=0,  # Number of epochs and verbosity level\n",
        "                            callbacks=[callback], validation_split=0.2, shuffle=True)  # Validation split and data shuffling\n",
        "\n",
        "        return history, model  # Return the training history and the trained model\n",
        "\n",
        "    except Exception as e:\n",
        "        # Print the error message if an exception occurs\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0qmLK4xlMij"
      },
      "source": [
        "# Generating results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9CbK9bejGRB"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Import NumPy for numerical operations\n",
        "import matplotlib.pyplot as plt  # Import Matplotlib for plotting\n",
        "from sklearn.metrics import classification_report, confusion_matrix  # Import metrics for model evaluation\n",
        "\n",
        "def result(x, y, history, model):\n",
        "    \"\"\"\n",
        "    Generate and print the classification report, confusion matrix, and plot accuracy and loss.\n",
        "\n",
        "    Parameters:\n",
        "        x (numpy.ndarray): Input features.\n",
        "        y (numpy.ndarray): Target labels.\n",
        "        history (keras.History): Training history of the model.\n",
        "        model (keras.Model): Trained model.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"+++ Generating result... +++\")\n",
        "\n",
        "        # Predict on test data\n",
        "        pred = model.predict(x)\n",
        "\n",
        "        # Evaluate model performance on test data\n",
        "        score = model.evaluate(x, y, verbose=0)\n",
        "        print(\"------------- Neural Network Classification ----------------\")\n",
        "        print('Test loss:', score[0])\n",
        "        print('Test accuracy:', score[1])\n",
        "\n",
        "        # Convert predictions and ground truth labels to binary classification\n",
        "        prediction = np.argmax(pred, axis=1)  # Get the index of the maximum value in each prediction\n",
        "        real = np.argmax(y, axis=1)  # Get the index of the maximum value in each true label\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"-----------------------------\" * 3)\n",
        "        print(\"--------------------- Classification Report -----------------------\")\n",
        "        print(\"-----------------------------\" * 3)\n",
        "        print(classification_report(real, prediction))\n",
        "\n",
        "        # Print confusion matrix\n",
        "        print(\"-----------------------------\" * 3)\n",
        "        print(\"--------------------- Confusion Matrix ----------------\")\n",
        "        print(\"-----------------------------\" * 3)\n",
        "        conf_mat = confusion_matrix(real, prediction)\n",
        "        print(conf_mat)\n",
        "        print(\"-----------------------------\" * 3)\n",
        "\n",
        "        print(\"--------- Plot Accuracy and Loss ------\", \"\\n\")\n",
        "\n",
        "        # Plot accuracy and loss\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        # Plot model accuracy\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'], label='Train')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Plot model loss\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'], label='Train')\n",
        "        plt.plot(history.history['val_loss'], label='Validation')\n",
        "        plt.title('Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        plt.show()  # Display the plots\n",
        "\n",
        "        return score[1], score[0]  # Return the test accuracy and loss\n",
        "\n",
        "    except Exception as e:\n",
        "        # Print the error message if an exception occurs\n",
        "        print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgTtRGY4nn3c"
      },
      "source": [
        "# Deepfack Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjSd5XFKlCW8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "def Face_Fake_Detection(dataset_name,feature_kind,model_name):\n",
        "    # Lists to store loss and accuracy for each iteration\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for i in range(15):\n",
        "        \"\"\"\n",
        "        Perform deepfake detection using the specified model and dataset features.\n",
        "\n",
        "        Parameters:\n",
        "            model_name (str): Name of the model to be used.\n",
        "            dataset_name (str): Name of the dataset.\n",
        "            feature_kind (str): Type of features to be used.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple containing the accuracy and loss scores of the model.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Clear previous Keras session to free up memory\n",
        "            keras.backend.clear_session()\n",
        "\n",
        "            # Load features from the dataset\n",
        "            real_feature, fake_feature = feature_loader(dataset_name, model_name, feature_kind)\n",
        "            if real_feature is None or fake_feature is None:\n",
        "                print(\"There is something wrong during loading features....\")\n",
        "                break\n",
        "\n",
        "            # Label the features as real (0) and fake (1)\n",
        "            print(\"Labeling features...\")\n",
        "            data, label = labeling(real_feature, fake_feature)\n",
        "            del real_feature, fake_feature\n",
        "\n",
        "            # Split the labeled data into training and testing sets\n",
        "            print(\"Splitting data into train and test sets...\")\n",
        "            X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42, shuffle=True)\n",
        "            del data, label\n",
        "\n",
        "            # Define the input shape for the neural network\n",
        "            input_shape = X_train.shape[1:]\n",
        "\n",
        "            # Train the neural network classifier and measure training time\n",
        "            start_time = time.process_time()\n",
        "            history, model = neural_network_classification(X_train, y_train, input_shape)\n",
        "            print(\"Training time:\", time.process_time() - start_time)\n",
        "\n",
        "            # Evaluate the model's performance on the test set\n",
        "            y_test = to_categorical(y_test)\n",
        "            accuracy, loss = result(X_test, y_test, history, model)\n",
        "\n",
        "            # Clean up memory\n",
        "            del X_train, y_train, X_test, y_test\n",
        "\n",
        "            # Append the accuracy and loss to the respective lists\n",
        "            accuracies.append(accuracy)\n",
        "            losses.append(loss)\n",
        "\n",
        "            # Clear the session to free up memory\n",
        "            keras.backend.clear_session()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    return  accuracies,losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7JcLf7GoF1P"
      },
      "source": [
        "# ***variabels:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfGYosCPDnZx"
      },
      "outputs": [],
      "source": [
        "#variabels:\n",
        "dataset_name = '140k'\n",
        "feature_kind=\"correlation\" #correlation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of valid options for the model_name\n",
        "valid_options_model_name = [\"vgg16\", \"vgg19\", \"resnet50\"]\n",
        "\n",
        "# Prompting the user for the first input and validating it\n",
        "while True:\n",
        "    model_name = input(\"Enter one of the following options for the model_name: vgg16, vgg19, resnet50:\").strip().lower()\n",
        "    if model_name in valid_options_model_name:\n",
        "        print(\"Valid input for the model_name detected:\", model_name)\n",
        "        break  # Break out of the loop if input is valid\n",
        "    else:\n",
        "        print(\"Invalid input. Please enter one of the specified options.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMgOB7s01Q-6",
        "outputId": "2bd5841b-9ab9-4b00-f625-cd6c755b7994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter one of the following options for the model_name: vgg16, vgg19, resnet50:vgg16\n",
            "Valid input for the model_name detected: vgg16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbshejEFXW4h"
      },
      "outputs": [],
      "source": [
        "accuracy, loss=Face_Fake_Detection(dataset_name,feature_kind,model_name)\n",
        "print(\"-------------------\"*5)\n",
        "print(\"+++ mean ... +++\")\n",
        "acc_mean= np.mean(accuracy)\n",
        "loss_mean=np.mean(loss)\n",
        "print(acc_mean)\n",
        "print(loss_mean)\n",
        "\n",
        "print(\"-------------------\"*5)\n",
        "print(\"+++ median ... +++\")\n",
        "median_acc = np.median(accuracy)\n",
        "median_loss = np.median(loss)\n",
        "print(median_acc)\n",
        "print(median_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
